{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3.7.9 64-bit (conda)","metadata":{"interpreter":{"hash":"b4aec9ca89c154523482d0265d5b720ead2adae67fece8b0581c8740aa4389dd"}}},"colab":{"name":"g3_collecte_v0.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"kJfVvQyWRY2k"},"source":["Créé le 9 mars 2021\n","\n","**Projet Tableau de Bord** \n","\n","**Groupe n°3 - Arnaques en ligne**\n","\n","**Scrapping : récupération des articles**\n","\n","\n","@authors:\n","- VEDIS Theo \n","- MANSON Marianne\n","- KIRED Nour Elhouda\n"]},{"cell_type":"markdown","metadata":{"id":"cQOK0yOJPbxP"},"source":["#### Import libraries"]},{"cell_type":"code","metadata":{"id":"VE6fe5dNN37x"},"source":["########## Module import ##########\n","# Fichiers\n","import json\n","from tqdm.notebook import tqdm\n","from os import path \n","\n","# Scraping\n","\n","from bs4 import BeautifulSoup\n","import requests \n","import sys\n","\n","# Format\n","import  time\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qudTqrlWQSNL"},"source":["#### Fonctions"]},{"cell_type":"markdown","metadata":{"id":"BKf3oNJveivK"},"source":["écupération des titres / résumés de chaque liens"]},{"cell_type":"code","metadata":{"id":"HI-Z3dZcN374"},"source":["def article_url_to_data(url) -> dict:\n","    \"\"\"Documentation\n","    fonction qui scrape l'article et retourne les données collectées sous forme d'un dictionnaire\n","    Parameters:\n","            url: lien de l'article  dont on veut extraire les données\n","\n","      Out:\n","            dictionnaire contenant les éléments essentiels des articles \n","    \"\"\"\n","    try:\n","        r = requests.get(url)\n","    except:\n","        print(\"Error : \", sys.exc_info()[0])\n","        return {\n","            \"url\": url,\n","            \"abstract\": \"ERREUR_REQUEST\",\n","            \"author\": \"ERREUR_REQUEST\",\n","            \"keyword\": \"ERREUR_REQUEST\",\n","            \"title\": \"ERREUR_REQUEST\",\n","            \"date\": \"ERREUR_REQUEST\",\n","        }\n","\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","\n","    try:\n","        abstract = soup.find(\"div\", {\"id\": \"Abs1-section\"}).text\n","    except AttributeError:\n","        abstract = \"no_data\"\n","\n","    author = [\n","        i.find(\"span\", {\"itemprop\": \"name\"}).text\n","        for i in soup.find(\"article\").find_all(\"li\", {\"class\": \"c-author-list__item\"})\n","    ]\n","\n","    keyword = [\n","        i.text\n","        for i in soup.find(\"article\").find_all(\n","            \"li\", {\"class\": \"c-article-subject-list__subject\"}\n","        )\n","    ]\n","\n","    title = soup.title.text.replace(\" | SpringerLink\", \"\")\n","\n","    date = soup.find(\"article\").find(\"header\").find(\"time\")[\"datetime\"]\n","\n","    return {\n","        \"url\": url,\n","        \"abstract\": abstract,\n","        \"author\": author,\n","        \"keyword\": keyword,\n","        \"title\": title,\n","        \"date\": date,\n","    }\n","\n","\n","##############################################################################\n","def chapter_url_to_data(url) -> dict:\n","    \"\"\"Documentation\n","    fonction qui scrape un chapter et retourne les données collectées sous forme d'un dictionnaire\n","\n","      Parameters:\n","            url: lien du chapter dont on veut extraire les données\n","\n","      Out:\n","            dictionnaire contenant les éléments essentiels des articles \n","    \"\"\"\n","    try:\n","        r = requests.get(url)\n","    except:\n","        print(\"Error : \", sys.exc_info()[0])\n","        return {\n","            \"url\": url,\n","            \"abstract\": \"ERREUR_REQUEST\",\n","            \"author\": \"ERREUR_REQUEST\",\n","            \"keyword\": \"ERREUR_REQUEST\",\n","            \"title\": \"ERREUR_REQUEST\",\n","            \"date\": \"ERREUR_REQUEST\",\n","        }\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","\n","    try:\n","        abstract = (\n","            soup.find(\"section\", {\"id\": \"Abs1\"}).find(\"p\", {\"class\": \"Para\"}).text\n","        )\n","    except AttributeError:\n","        abstract = \"no_data\"\n","\n","    # print(abstract)\n","\n","    try:\n","        l = [\n","            i.text\n","            for i in soup.find(\"div\", {\"class\": \"authors-affiliations\"})\n","            .find(\"ul\")\n","            .find_all(\"li\")\n","        ]  # Author + num\n","    except AttributeError:\n","        l = []\n","\n","    author = []\n","    for i in range(int(len(l) / 2)):\n","        author.append((l[i * 2][:-1], l[i * 2 + 1]))\n","\n","    # Affiliation + author id\n","    try:\n","        l = [\n","            i.text\n","            for i in soup.find(\"div\", {\"class\": \"authors-affiliations\"})\n","            .find(\"ol\")\n","            .find_all(\"li\")\n","        ]\n","    except AttributeError:\n","        l = []\n","\n","    affiliations = []\n","    for i in l:\n","        splited_affiliations = i.split(\".\")\n","        affiliations.append(\n","            (splited_affiliations[0], \".\".join(splited_affiliations[1:]))\n","        )\n","\n","    author_final = {}\n","    for i in author:\n","        author_final[i[0]] = []\n","        for j in affiliations:\n","            if j[0] == i[1]:\n","                author_final[i[0]] += author_final[i[0]] + [j[1]]\n","\n","    # print(\"Author :\", author_final)\n","\n","    try:\n","        keyword = [\n","            i.text\n","            for i in soup.find(\"div\", {\"class\": \"KeywordGroup\"}).find_all(\n","                \"span\", {\"class\": \"Keyword\"}\n","            )\n","        ]\n","    except AttributeError:\n","        keyword = []\n","\n","    title = soup.title.text.replace(\" | SpringerLink\", \"\")\n","\n","    date = \"-\".join(\n","        [\n","            i if len(i) > 1 else \"0\" + i\n","            for i in soup.find(\"meta\", {\"name\": \"citation_publication_date\"})[\n","                \"content\"\n","            ].split(\"/\")\n","        ]\n","    )\n","\n","    if len(date) != 10:\n","        try:\n","            date = soup.find(\"div\", {\"class\": \"article-dates\"}).find(\"time\")[\"datetime\"]\n","        except AttributeError:\n","            try:\n","                date = soup.find(\"meta\", {\"name\": \"citation_publication_date\"})[\n","                    \"content\"\n","                ]\n","            except AttributeError:\n","                date = \"no_data\"\n","\n","    return {\n","        \"url\": url,\n","        \"abstract\": abstract,\n","        \"author\": author_final,\n","        \"keyword\": keyword,\n","        \"title\": title,\n","        \"date\": date,\n","    }\n","\n","\n","##############################################################################\n","# Full fct Reference Work Entry\n","def workEntry_url_to_data(url) -> dict:\n","    \"\"\"Documentation\n","    fonction qui scrape un work entry et retourne les données collectées sous forme d'un dictionnaire\n","\n","      Parameters:\n","            url: lien de work entry dont on veut extraire les données\n","\n","      Out:\n","            dictionnaire contenant les éléments essentiels des articles \n","    \"\"\"\n","    try:\n","        r = requests.get(url)\n","    except:\n","        print(\"Error : \", sys.exc_info()[0])\n","        return {\n","            \"url\": url,\n","            \"abstract\": \"ERREUR_REQUEST\",\n","            \"author\": \"ERREUR_REQUEST\",\n","            \"keyword\": \"ERREUR_REQUEST\",\n","            \"title\": \"ERREUR_REQUEST\",\n","            \"date\": \"ERREUR_REQUEST\",\n","        }\n","\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","\n","    abstract = soup.find(\"meta\", {\"name\": \"description\"})[\"content\"]\n","\n","    l = [\n","        i.text\n","        for i in soup.find(\"div\", {\"class\": \"authors-affiliations\"})\n","        .find(\"ul\")\n","        .find_all(\"li\")\n","    ]  # Author + num\n","\n","    author = []\n","    for i in range(int(len(l) / 2)):\n","        author.append((l[i * 2][:-1], l[i * 2 + 1]))\n","\n","    try:\n","        l = [\n","            i.text\n","            for i in soup.find(\"div\", {\"class\": \"authors-affiliations\"})\n","            .find(\"ol\")\n","            .find_all(\"li\")\n","        ]\n","    except AttributeError:\n","        l = []\n","\n","    affiliations = []\n","    for i in l:\n","        splited_affiliations = i.split(\".\")\n","        affiliations.append(\n","            (splited_affiliations[0], \".\".join(splited_affiliations[1:]))\n","        )\n","\n","    author_final = {}\n","    for i in author:\n","        author_final[i[0]] = []\n","        for j in affiliations:\n","            if j[0] == i[1]:\n","                author_final[i[0]] += author_final[i[0]] + [j[1]]\n","\n","    try:\n","        keyword = [\n","            i.text\n","            for i in soup.find(\"div\", {\"class\": \"KeywordGroup\", \"lang\": \"en\"}).findAll(\n","                \"span\", {\"class\": \"Keyword\"}\n","            )\n","        ]\n","    except AttributeError:\n","        keyword = []\n","\n","    title = soup.title.text.replace(\" | SpringerLink\", \"\")\n","\n","    try:\n","        date = soup.find(\"div\", {\"class\": \"article-dates\"}).find(\"time\")[\"datetime\"]\n","    except AttributeError:\n","        try:\n","            date = soup.find(\"meta\", {\"name\": \"citation_publication_date\"})[\"content\"]\n","        except AttributeError:\n","            date = \"no_data\"\n","\n","    return {\n","        \"url\": url,\n","        \"abstract\": abstract,\n","        \"author\": author_final,\n","        \"keyword\": keyword,\n","        \"title\": title,\n","        \"date\": date,\n","    }\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7IRdB6sZjFt3"},"source":["#### Collecte et sauvegrade des données "]},{"cell_type":"code","metadata":{"id":"Dm5pAfYYN377"},"source":["def urls_to_type(url: str) -> str:\n","    \"\"\"Documentation\n","    Parameters:\n","        url: springer article url\n","    Out:\n","        type de l'url \n","\n","    \"\"\"\n","    return url.split(\"/\")[3]\n","\n","\n","def collect(urls: zip) -> None:\n","    \"\"\"Documentation\n","    fonction qui reçoit en paramètres les liens d'articles et qui extrairait par la suite \n","    les données qu'on sauvegardera sous format json    \n","\n","    Parameters:\n","        urls: Liste des urls \n","    Out:\n","        data: creation d'un fichier json contenant toutes les données collectées\n","    \"\"\"\n","\n","    data = {}\n","\n","    for id, url in tqdm(zip(range(len(urls)), urls), total=len(urls)):\n","        article_type = urls_to_type(url)\n","        if article_type == \"referenceworkentry\":\n","            try:\n","                data[id] = workEntry_url_to_data(url)\n","            except:\n","                print(\"ERROR-WE :\", sys.exc_info()[0])\n","                data[id] = {\n","                    \"url\": url,\n","                    \"abstract\": \"ERROR\",\n","                    \"author\": \"ERROR\",\n","                    \"keyword\": \"ERROR\",\n","                    \"title\": \"ERROR\",\n","                    \"date\": \"ERROR\",\n","                }\n","        elif article_type == \"article\":\n","            try:\n","                data[id] = article_url_to_data(url)\n","            except:\n","                print(\"ERROR-ART :\", sys.exc_info()[0])\n","                data[id] = {\n","                    \"url\": url,\n","                    \"abstract\": \"ERROR\",\n","                    \"author\": \"ERROR\",\n","                    \"keyword\": \"ERROR\",\n","                    \"title\": \"ERROR\",\n","                    \"date\": \"ERROR\",\n","                }\n","        elif article_type == \"chapter\":\n","            try:\n","                data[id] = chapter_url_to_data(url)\n","            except:\n","                print(\"ERROR-CHAP :\", sys.exc_info()[0])\n","                data[id] = {\n","                    \"url\": url,\n","                    \"abstract\": \"ERROR\",\n","                    \"author\": \"ERROR\",\n","                    \"keyword\": \"ERROR\",\n","                    \"title\": \"ERROR\",\n","                    \"date\": \"ERROR\",\n","                }\n","\n","    with open(\".\\data.json\", \"w\") as f:\n","        json.dump(data, f)\n"],"execution_count":null,"outputs":[]}]}