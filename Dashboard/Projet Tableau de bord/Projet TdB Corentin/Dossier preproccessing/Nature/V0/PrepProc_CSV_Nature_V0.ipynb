{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Pour le preproccess\n",
    "import spacy as sp\n",
    "nlp = sp.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a décidé de ne prendre que 15 sites (cf. Stats_Desc_Nature_V0.ipynb):\n",
    "\n",
    "list_chosen_site = ['Scientific Reports',\n",
    "                    'Nature Communications',\n",
    "                    'Cell Death & Disease',\n",
    "                    'Oncogene',\n",
    "                    'Nature',\n",
    "                    'Neuropsychopharmacology',\n",
    "                    'British Journal of Cancer',\n",
    "                    'Leukemia',\n",
    "                    'Acta Pharmacologica Sinica',\n",
    "                    'Nature Medicine',\n",
    "                    'Translational Psychiatry',\n",
    "                    'Nature Neuroscience',\n",
    "                    'Molecular Psychiatry',\n",
    "                    'Nature Protocols',\n",
    "                    'Cell Death & Differentiation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement des CSV en un (à opti)\n",
    "df_all = pd.concat([pd.read_csv(\"../Data/Nature/articles_Nature_data_1_V0.csv\", sep=\";\"),\n",
    "                    pd.read_csv(\"../Data/Nature/articles_Nature_data_2_V0.csv\", sep=\";\"),\n",
    "                    pd.read_csv(\"../Data/Nature/articles_Nature_data_3_V0.csv\", sep=\";\"),\n",
    "                    pd.read_csv(\"../Data/Nature/articles_Nature_data_fin_V0.csv\", sep=\";\")])\n",
    "\n",
    "\n",
    "#Suppression de la colonne doi et de l'indice des articles qui est en double\n",
    "df_all = df_all.drop(df_all.columns[[0, 1]], axis='columns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si trop lourd pour la RAM, en charger un, le préparer nettoyer etc puis n'enregistrer que cela.\n",
    "\n",
    "Si toujours pas suffisant, il faudra faire les \"réductions de dimensions\" (IDF etc) séparemment et trouver un moyen de les assemblées ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_chosen_site(df_all:pd.DataFrame, list_chosen_site) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "        Desc. : La fonction permet de ne garder que les articles provenant des sites choisis\n",
    "        \n",
    "        Paramètre :\n",
    "            df_all : le dataframe de tous les CVS\n",
    "            list_chosen_site : la liste des noms des sites choisis\n",
    "            \n",
    "        Out :\n",
    "            le nouveau dataFrame avec uniquement les sites voulus\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    listeOK = []\n",
    "    for i in df_all.itertuples():\n",
    "        listeOK.append(i.site in list_chosen_site)\n",
    "\n",
    "    df_all['test'] = listeOK\n",
    "    df_all = df_all[df_all['test']]\n",
    "    df_all = df_all.drop(columns='test')\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = get_only_chosen_site(df_all, list_chosen_site)\n",
    "#np.unique(df_all['site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDate(df: pd.DataFrame, date_column: str, taux_nan: float):\n",
    "    \"\"\"\n",
    "    Fonction : \n",
    "        - Nettoyage des lignes avec des champs non référencés si < tauxNan.\n",
    "        - Formatage des dates au formats MM YYYY\n",
    "    \n",
    "    \n",
    "    Paramètres :\n",
    "        df : le dataFrame issu des CSV Nature\n",
    "        date_column : la nom de la conlonne contenant les dates\n",
    "        taux_nan : le taux de lignes contenant des nan en dessous duquel on accepte la suppression des lignes\n",
    "    \n",
    "    Sorties :\n",
    "        N.A. (modification du dataframe fournit en entrée)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Suppression des lignes avec nan si inf à taux_nan du nb d'article tt + recalcul du nbArticle\n",
    "    ligneNan = df.index[df.isnull().any(axis=1)]\n",
    "    nbLigneNan = len(ligneNan)\n",
    "    nbArct = df.shape[0]\n",
    "    \n",
    "    if nbLigneNan != 0:\n",
    "        if nbLigneNan < taux_nan * nbArct:\n",
    "            df = df.drop(ligneNan, 0, inplace=True)\n",
    "            nbArct = df.shape[0]   \n",
    "    \n",
    "    \n",
    "    #Corr° des dates\n",
    "    corrDate = [('January', '01'),\n",
    "                ('February', '02'),\n",
    "                ('March', '03'),\n",
    "                ('April', '04'),\n",
    "                ('May', '05'),\n",
    "                ('June', '06'),\n",
    "                ('July', '07'),\n",
    "                ('August', '08'),\n",
    "                ('September', '09'),\n",
    "                ('October', '10'),\n",
    "                ('November', '11'),\n",
    "                ('December', '12')]\n",
    "\n",
    "    cleanDate = []\n",
    "    date = np.array(df[date_column])\n",
    "    \n",
    "    for i in date:\n",
    "        try:\n",
    "            if type(int(i[0:2])) is int:\n",
    "                dateTemp = i[3:]\n",
    "\n",
    "        except:\n",
    "            dateTemp = i\n",
    "\n",
    "        for k, v in corrDate:\n",
    "            if dateTemp.replace(k, v) != dateTemp:\n",
    "                cleanDate.append(dateTemp.replace(k, v))\n",
    "\n",
    "                \n",
    "    if len(cleanDate) != 0:\n",
    "        df[date_column] = cleanDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDate(df_all, 'date', 0.1)\n",
    "#df_all[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Desc. :\n",
    "        Cette fonction permet de nettoyer le texte fournit par suppr des stop_word, lemmatization etc.\n",
    "\n",
    "    Parametre :\n",
    "        text: issus par exp de df.iloc[0]['text']\n",
    "\n",
    "    Out :\n",
    "        Ressort le texte nettoyé\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text = nlp(text)\n",
    "    text_clean = []\n",
    "\n",
    "    for token in text:\n",
    "        if token.is_stop == False and token.pos_ not in ['PUNCT', 'SPACE'] and token.is_alpha == True:\n",
    "            text_clean.append(token.lemma_)\n",
    "\n",
    "    return (\" \".join(text_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_all.iloc[2]['text']\n",
    "print(text[0:200], \"\\n\")\n",
    "text_nett = clean_text(text)\n",
    "print(text_nett[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nettoyage des textes (long)\n",
    "all_text_clean = []\n",
    "\n",
    "for i in tqdm(df_all.itertuples()):\n",
    "    text_clean = clean_text(i.text)\n",
    "    all_text_clean.append(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On ecrase les anciens text par les cleans\n",
    "df_all['text'] = all_text_clean\n",
    "\n",
    "#df_all[0:5]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir avec le scrappe si on ne peut pas être plus précis (récupérer le type, l'auteur etc à part et ne garder que les abstracts des textes par exp) ==> V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1167px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
