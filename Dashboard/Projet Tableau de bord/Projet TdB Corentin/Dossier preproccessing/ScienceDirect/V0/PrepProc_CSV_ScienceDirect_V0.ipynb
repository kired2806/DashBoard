{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy as sp\n",
    "nlp = sp.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date_science(df: pd.DataFrame, date_column: str) -> list:\n",
    "    \"\"\"\n",
    "    Formatting dates in MM YYYY formats\n",
    "    \n",
    "    \n",
    "    In :\n",
    "        df: the dataFrame from CSV Nature\n",
    "        date_column: the name of the column containing the dates\n",
    "    \n",
    "    Out :\n",
    "        date column\n",
    "    \n",
    "    \"\"\"\n",
    "    col_date = df[date_column]\n",
    "    nb_date = len(col_date)\n",
    "    \n",
    "    \n",
    "    # On ne garde que le mois (ou période) et l'année + séparation des dates en 2 parties (exp : Nov-Dec)\n",
    "    col_date = [\" \".join(i.split(\"–\")[-1].split()[-2:]) for i in col_date]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # On remplace les valeurs qui ne sont pas des dates (dernier élément =! int) par des vides\n",
    "    for i in range(nb_date):\n",
    "        if len(col_date[i].split()) == 2 :  \n",
    "            try:\n",
    "                if type(int(col_date[i].split()[-1])) is int:\n",
    "                    pass\n",
    "\n",
    "            except:\n",
    "                col_date[i] = \"\"\n",
    "        else:\n",
    "            col_date[i] = \"\"\n",
    "               \n",
    "    \n",
    "    return col_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(col_text: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Cleaning the text column from the provided dataframe\n",
    "\n",
    "    In :\n",
    "        col_text: the column of provided dataframe with the text\n",
    "\n",
    "    Out :\n",
    "        Give column ( <=> list) with the cleaning text\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    col_clean_text = []\n",
    "    for i in tqdm(col_text):\n",
    "        text = nlp(i)\n",
    "        text_clean = []\n",
    "\n",
    "        for token in text:\n",
    "            if token.is_stop == False and token.pos_ not in ['PUNCT', 'SPACE'] and token.is_alpha == True:\n",
    "                text_clean.append(token.lemma_)\n",
    "\n",
    "        col_clean_text.append(\" \".join(text_clean))\n",
    "        \n",
    "    return col_clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_freq_words(tfidf_vect, mat_tfidf, n_terms: int) -> list: \n",
    "    \"\"\"\n",
    "    This function allows you to find the n_terms most important words ( <=> most frequent) of each article.\n",
    "\n",
    "    In :\n",
    "        mat_tfidf:  tf_idf of articles\n",
    "        n_terms: the wanted number of term\n",
    "        \n",
    "    Out :\n",
    "        the column with the most frequent words\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    freq_word = []\n",
    "    labels = tfidf_vect.get_feature_names()\n",
    "    \n",
    "    for l in tqdm(range(mat_tfidf.shape[0])):\n",
    "        ligne = pd.DataFrame(mat_tfidf[l].todense())\n",
    "        \n",
    "        for i,r in ligne.iterrows():\n",
    "            freq_word.append([','.join([labels[t] for t in np.argsort(r)[-n_terms:]])])\n",
    "            \n",
    "    return freq_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV\n",
    "df_all = pd.read_csv(\"../../../Data/Science Direct/Not_Clean/articles_sciencedirect_V0.csv\", sep=\";\")\n",
    "df_all = df_all.drop(df_all.columns[[0]], axis='columns')\n",
    "print(\"Chargement OK \\n\")\n",
    "\n",
    "# lower on text column\n",
    "df_all['text'] = df_all['text'].str.lower()\n",
    "print(\"lower texts OK \\n\")\n",
    "\n",
    "# Preprocess of dates\n",
    "df_all['date'] = clean_date_science(df_all, 'date')\n",
    "print(\"Dates OK \\n\")\n",
    "\n",
    "# Separation of the text column into its 3 components: highlights, abstract, keywords \n",
    "# + We remove the abstract etc. at the beginning.\n",
    "df_all = pd.concat([df_all, \n",
    "                    pd.DataFrame([i[1].split('%%$%%') for i in df_all['text'].iteritems()], \n",
    "                                 columns = ['highlight', 'abstract', 'keyword'], \n",
    "                                 index = df_all.index)], \n",
    "                   sort=False, axis = 1)\n",
    "\n",
    "df_all['highlight'] = [i[11:] for i in df_all['highlight']]\n",
    "df_all['abstract'] = [i[8:] for i in df_all['abstract']]\n",
    "print(\"Separation text OK \\n\")\n",
    "\n",
    "\n",
    "# Cleaning of highlights (10 min)\n",
    "df_all['highlight'] = clean_text(df_all['highlight'])\n",
    "print(\"Cleaning highlight OK \\n\")\n",
    "\n",
    "# Cleaning of abstracts (75 min)\n",
    "df_all['abstract'] = clean_text(df_all['abstract'])\n",
    "print(\"Cleaning abstract OK \\n\")\n",
    "\n",
    "# TF-IDF for articles without keyword sections (on highlight + abstract grouped together)\n",
    "tfidf_vect = TfidfVectorizer(lowercase=False, stop_words=None)\n",
    "X = tfidf_vect.fit_transform(df_all[df_all['keyword']==\"\"]['highlight'] + df_all[df_all['keyword']==\"\"]['abstract'])\n",
    "print(\"TF-IDF OK \\n\")\n",
    "\n",
    "# Keyword per article (1 min)\n",
    "df_all[df_all['keyword']==\"\"]['keyword'] = get_most_freq_words(tfidf_vect, X, 5)\n",
    "print(\"Keywords OK \\n\")\n",
    "\n",
    "# Export CSV\n",
    "df_all.to_csv('../../../Data/Science Direct/Clean/articles_sciencedirect_clean_V0.csv', sep = \";\", index = True)\n",
    "print(\"Export OK \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1167px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
